{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Brain Tumor Detection using U-Net\n",
        "\n",
        "This notebook implements a U-Net architecture for brain tumor segmentation using the LGG MRI Segmentation Dataset.\n",
        "\n",
        "## Project Overview\n",
        "*   **Task**: Semantic Segmentation of Brain Tumors\n",
        "*   **Model**: U-Net (Encoder-Decoder with Skip Connections)\n",
        "*   **Dataset**: LGG MRI Segmentation Dataset\n",
        "*   **Framework**: TensorFlow/Keras\n",
        "\n",
        "## 1. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import kagglehub\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Configuration\n",
        "IM_HEIGHT = 256\n",
        "IM_WIDTH = 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 1e-4\n",
        "SMOOTH = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Loading and Preprocessing\n",
        "\n",
        "We download the dataset using `kagglehub` and create a DataFrame containing paths to images and their corresponding masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"mateuszbuda/lgg-mri-segmentation\")\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "# Initialize lists to store paths\n",
        "image_paths = []\n",
        "mask_paths = []\n",
        "\n",
        "# Walk through the directory to find images and masks\n",
        "for dirpath, dirnames, filenames in os.walk(path):\n",
        "    for filename in filenames:\n",
        "        if 'mask' not in filename and filename.endswith('.tif'):\n",
        "            image_path = os.path.join(dirpath, filename)\n",
        "            mask_path = os.path.join(dirpath, filename.replace('.tif', '_mask.tif'))\n",
        "            \n",
        "            # Check if mask exists\n",
        "            if os.path.exists(mask_path):\n",
        "                image_paths.append(image_path)\n",
        "                mask_paths.append(mask_path)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({'image_path': image_paths, 'mask_path': mask_paths})\n",
        "print(f\"Total images found: {len(df)}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "Split the data into Training, Validation, and Test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into Train+Val and Test\n",
        "df_train_val, df_test = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "\n",
        "# Split Train+Val into Train and Validation\n",
        "df_train, df_val = train_test_split(df_train_val, test_size=0.2, random_state=SEED)\n",
        "\n",
        "print(f\"Training set: {len(df_train)}\")\n",
        "print(f\"Validation set: {len(df_val)}\")\n",
        "print(f\"Test set: {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Generators\n",
        "We use `ImageDataGenerator` for data augmentation (on training set) and rescaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generator(data_frame, batch_size, aug_dict, image_color_mode=\"rgb\",\n",
        "                   mask_color_mode=\"grayscale\", image_save_prefix=\"image\",\n",
        "                   mask_save_prefix=\"mask\", save_to_dir=None, target_size=(256, 256), seed=1):\n",
        "    \n",
        "    image_datagen = ImageDataGenerator(**aug_dict)\n",
        "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "    \n",
        "    image_generator = image_datagen.flow_from_dataframe(\n",
        "        data_frame,\n",
        "        x_col=\"image_path\",\n",
        "        class_mode=None,\n",
        "        color_mode=image_color_mode,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        save_to_dir=save_to_dir,\n",
        "        save_prefix=image_save_prefix,\n",
        "        seed=seed)\n",
        "\n",
        "    mask_generator = mask_datagen.flow_from_dataframe(\n",
        "        data_frame,\n",
        "        x_col=\"mask_path\",\n",
        "        class_mode=None,\n",
        "        color_mode=mask_color_mode,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        save_to_dir=save_to_dir,\n",
        "        save_prefix=mask_save_prefix,\n",
        "        seed=seed)\n",
        "\n",
        "    train_gen = zip(image_generator, mask_generator)\n",
        "    \n",
        "    for (img, mask) in train_gen:\n",
        "        img, mask = adjust_data(img, mask)\n",
        "        yield (img, mask)\n",
        "\n",
        "def adjust_data(img, mask):\n",
        "    img = img / 255.0\n",
        "    mask = mask / 255.0\n",
        "    mask[mask > 0.5] = 1\n",
        "    mask[mask <= 0.5] = 0\n",
        "    return (img, mask)\n",
        "\n",
        "# Augmentation parameters for training\n",
        "train_generator_args = dict(rotation_range=0.2,\n",
        "                            width_shift_range=0.05,\n",
        "                            height_shift_range=0.05,\n",
        "                            shear_range=0.05,\n",
        "                            zoom_range=0.05,\n",
        "                            horizontal_flip=True,\n",
        "                            fill_mode='nearest')\n",
        "\n",
        "# Create generators\n",
        "train_gen = data_generator(df_train, BATCH_SIZE, train_generator_args, target_size=(IM_HEIGHT, IM_WIDTH))\n",
        "val_gen = data_generator(df_val, BATCH_SIZE, dict(), target_size=(IM_HEIGHT, IM_WIDTH))\n",
        "test_gen = data_generator(df_test, BATCH_SIZE, dict(), target_size=(IM_HEIGHT, IM_WIDTH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Metrics and Loss Functions\n",
        "We use Dice Coefficient and Intersection over Union (IoU) as metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coefficients(y_true, y_pred, smooth=SMOOTH):\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coefficients_loss(y_true, y_pred):\n",
        "    return -dice_coefficients(y_true, y_pred)\n",
        "\n",
        "def iou(y_true, y_pred, smooth=SMOOTH):\n",
        "    intersection = tf.keras.backend.sum(y_true * y_pred)\n",
        "    sum_ = tf.keras.backend.sum(y_true + y_pred)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return jac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. U-Net Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unet(input_size=(256, 256, 3)):\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    # Encoder\n",
        "    conv1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
        "    bn1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, (3, 3), padding='same')(bn1)\n",
        "    bn1 = BatchNormalization(axis=3)(conv1)\n",
        "    bn1 = Activation('relu')(bn1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n",
        "\n",
        "    conv2 = Conv2D(128, (3, 3), padding='same')(pool1)\n",
        "    bn2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, (3, 3), padding='same')(bn2)\n",
        "    bn2 = BatchNormalization(axis=3)(conv2)\n",
        "    bn2 = Activation('relu')(bn2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
        "\n",
        "    conv3 = Conv2D(256, (3, 3), padding='same')(pool2)\n",
        "    bn3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, (3, 3), padding='same')(bn3)\n",
        "    bn3 = BatchNormalization(axis=3)(conv3)\n",
        "    bn3 = Activation('relu')(bn3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
        "\n",
        "    conv4 = Conv2D(512, (3, 3), padding='same')(pool3)\n",
        "    bn4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, (3, 3), padding='same')(bn4)\n",
        "    bn4 = BatchNormalization(axis=3)(conv4)\n",
        "    bn4 = Activation('relu')(bn4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)\n",
        "\n",
        "    # Bridge\n",
        "    conv5 = Conv2D(1024, (3, 3), padding='same')(pool4)\n",
        "    bn5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(1024, (3, 3), padding='same')(bn5)\n",
        "    bn5 = BatchNormalization(axis=3)(conv5)\n",
        "    bn5 = Activation('relu')(bn5)\n",
        "\n",
        "    # Decoder\n",
        "    up6 = concatenate([Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bn5), conv4], axis=3)\n",
        "    conv6 = Conv2D(512, (3, 3), padding='same')(up6)\n",
        "    bn6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(512, (3, 3), padding='same')(bn6)\n",
        "    bn6 = BatchNormalization(axis=3)(conv6)\n",
        "    bn6 = Activation('relu')(bn6)\n",
        "\n",
        "    up7 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bn6), conv3], axis=3)\n",
        "    conv7 = Conv2D(256, (3, 3), padding='same')(up7)\n",
        "    bn7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(256, (3, 3), padding='same')(bn7)\n",
        "    bn7 = BatchNormalization(axis=3)(conv7)\n",
        "    bn7 = Activation('relu')(bn7)\n",
        "\n",
        "    up8 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(bn7), conv2], axis=3)\n",
        "    conv8 = Conv2D(128, (3, 3), padding='same')(up8)\n",
        "    bn8 = Activation('relu')(conv8)\n",
        "    conv8 = Conv2D(128, (3, 3), padding='same')(bn8)\n",
        "    bn8 = BatchNormalization(axis=3)(conv8)\n",
        "    bn8 = Activation('relu')(bn8)\n",
        "\n",
        "    up9 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(bn8), conv1], axis=3)\n",
        "    conv9 = Conv2D(64, (3, 3), padding='same')(up9)\n",
        "    bn9 = Activation('relu')(conv9)\n",
        "    conv9 = Conv2D(64, (3, 3), padding='same')(bn9)\n",
        "    bn9 = BatchNormalization(axis=3)(conv9)\n",
        "    bn9 = Activation('relu')(bn9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(bn9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "model = unet()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), \n",
        "              loss=dice_coefficients_loss, \n",
        "              metrics=[\"binary_accuracy\", iou, dice_coefficients])\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint('unet_brain_tumor.keras', verbose=1, save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=len(df_train) // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=len(df_val) // BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['dice_coefficients'], label='Train Dice')\n",
        "plt.plot(history.history['val_dice_coefficients'], label='Val Dice')\n",
        "plt.title('Dice Coefficient')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "model = load_model('unet_brain_tumor.keras', \n",
        "                   custom_objects={'dice_coefficients_loss': dice_coefficients_loss, \n",
        "                                   'iou': iou, \n",
        "                                   'dice_coefficients': dice_coefficients})\n",
        "\n",
        "results = model.evaluate(test_gen, steps=len(df_test) // BATCH_SIZE)\n",
        "print(\"Test Loss:\", results[0])\n",
        "print(\"Test IoU:\", results[2])\n",
        "print(\"Test Dice:\", results[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Predictions\n",
        "for i in range(5):\n",
        "    index = np.random.randint(0, len(df_test))\n",
        "    row = df_test.iloc[index]\n",
        "    \n",
        "    img = cv2.imread(row['image_path'])\n",
        "    img = cv2.resize(img, (IM_HEIGHT, IM_WIDTH))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    \n",
        "    mask = cv2.imread(row['mask_path'], cv2.IMREAD_GRAYSCALE)\n",
        "    mask = cv2.resize(mask, (IM_HEIGHT, IM_WIDTH))\n",
        "    mask = mask / 255.0\n",
        "    \n",
        "    pred = model.predict(img)[0]\n",
        "    pred = np.squeeze(pred)\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(np.squeeze(img))\n",
        "    plt.title('Image')\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title('True Mask')\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(pred, cmap='gray')\n",
        "    plt.title('Predicted Mask')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}